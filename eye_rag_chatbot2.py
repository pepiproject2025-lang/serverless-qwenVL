#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
eye_rag_chatbot2.py
-------------------
LangChain ReAct Agent ê¸°ë°˜ì˜ ì±—ë´‡ ëª¨ë“ˆ (Qwen3-VL + DuckDuckGo + Local Corpus)
"""

import os
import torch
from typing import Any, List, Optional, Dict
from dataclasses import dataclass, field

# LangChain & HuggingFace imports
from transformers import AutoModelForVision2Seq, AutoProcessor
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.prompts import PromptTemplate
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_community.tools import DuckDuckGoSearchResults
#from langchain_classic.agents import create_react_agent, AgentExecutor
# langchain_classicì´ ì•„ë‹ˆë¼ langchain.agentsë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
from langchain.agents import create_react_agent, AgentExecutor

# ----------------------------------
# 1) ì „ì—­ ì„¤ì • ë° ëª¨ë¸ ìºì‹± (Cold Start ë°©ì§€)
# ----------------------------------

MODEL_DIR = "/workspace/models/Qwen3_VL_8B_Instruct"  # ê²½ë¡œ í™•ì¸ í•„ìš”
CORPUS_DIR = "/workspace/corpus/"

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ì„ ì¡ì•„ë‘ì–´ í•¸ë“¤ëŸ¬ê°€ ì¬í˜¸ì¶œë  ë•Œ ë¦¬ë¡œë”© ë°©ì§€
_GLOBAL_MODEL = None
_GLOBAL_PROCESSOR = None

def load_global_model():
    global _GLOBAL_MODEL, _GLOBAL_PROCESSOR
    if _GLOBAL_MODEL is not None:
        return _GLOBAL_MODEL, _GLOBAL_PROCESSOR

    print(f"Loading Model from {MODEL_DIR}...")
    try:
        model = AutoModelForVision2Seq.from_pretrained(
            MODEL_DIR,
            device_map="auto",
            torch_dtype="auto",
            trust_remote_code=True
        ).eval()
        
        processor = AutoProcessor.from_pretrained(MODEL_DIR, trust_remote_code=True)
        print("Model Loaded Successfully!")
        
        _GLOBAL_MODEL = model
        _GLOBAL_PROCESSOR = processor
        return model, processor
    except Exception as e:
        print(f"Model Load Failed: {e}")
        raise e

# ----------------------------------
# 2) Custom LLM Wrapper (ë…¸íŠ¸ë¶ ì½”ë“œ ì ìš©)
# ----------------------------------
class QwenVLLLM(LLM):
    model: Any = None
    processor: Any = None

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        # Qwen-VL ì±„íŒ… í¬ë§· ì ìš©
        messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        inputs = self.processor(
            text=[text], images=None, videos=None, padding=True, return_tensors="pt"
        ).to(self.model.device)

        gen_kwargs = {
            "max_new_tokens": 1024, # ë‹µë³€ ê¸¸ì´ í™•ë³´
            "do_sample": True,
            "temperature": 0.1,     # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ ë‚®ì¶¤
            "repetition_penalty": 1.1,
            "top_p": 0.9,
            **kwargs
        }

        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, **gen_kwargs)
            
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        # Stop Token ì²˜ë¦¬
        if stop:
            for s in stop:
                if s in output_text:
                    output_text = output_text.split(s)[0]
        return output_text

    @property
    def _llm_type(self) -> str:
        return "qwen-vl-custom"

# ----------------------------------
# 3) Local Knowledge Loader (ë…¸íŠ¸ë¶ ì½”ë“œ ì ìš©)
# ----------------------------------
def load_local_knowledge(diagnosis_name: str) -> str:
    """
    ì§„ë‹¨ëª…(ì˜ˆ: ê²°ë§‰ì—¼)ì„ ì…ë ¥ë°›ì•„ /workspace/corpus/ê²°ë§‰ì—¼.txt íŒŒì¼ì„ ì½ì–´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    filename = f"{diagnosis_name}.txt"
    filepath = os.path.join(CORPUS_DIR, filename)
    
    try:
        if os.path.exists(filepath):
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
            print(f"[System] ë‚´ë¶€ ë¬¸ì„œ ë¡œë“œ ì„±ê³µ: {filename}")
            return content
        else:
            print(f"[System] ë‚´ë¶€ ë¬¸ì„œ ì—†ìŒ: {filename}")
            return "í•´ë‹¹ ì§ˆí™˜ì— ëŒ€í•œ ë‚´ë¶€ ìƒì„¸ ì§€ì¹¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
    except Exception as e:
        return f"ë‚´ë¶€ ë¬¸ì„œ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ----------------------------------
# 4) EyeRAGChatbot2 Class (LangChain Agent Encapsulation)
# ----------------------------------
@dataclass
class DogEyeCase:
    case_id: str
    diagnosis: str
    report_text: str
    symptoms: List[str] = field(default_factory=list)

class EyeRAGChatbot2:
    def __init__(self):
        # 1. ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹œ í™œìš©)
        model, processor = load_global_model()
        self.llm = QwenVLLLM(model=model, processor=processor)

        # 2. ë„êµ¬ ì„¤ì • (DuckDuckGo HTML backend - ì°¨ë‹¨ ìš°íšŒ)
        wrapper = DuckDuckGoSearchAPIWrapper(
            backend="html", 
            max_results=5
        )
        self.search_tool = DuckDuckGoSearchResults(api_wrapper=wrapper, source="text")
        self.tools = [self.search_tool]

        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë…¸íŠ¸ë¶ ìµœì‹  ë²„ì „)
        self.template = """
ë‹¹ì‹ ì€ **'ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ë°˜ë ¤ë™ë¬¼ ì•ˆê³¼ ìˆ˜ì˜ì‚¬ AI'**ì…ë‹ˆë‹¤.
ë³´í˜¸ìì˜ ê±±ì •ì— ê³µê°í•˜ë©°, [ì§„ë‹¨ ìš”ì•½]ê³¼ [ì›ë‚´ ì˜í•™ ì§€ì¹¨]ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

[ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬]
{tools}

[í˜•ì‹ ê°€ì´ë“œ - ì‹œìŠ¤í…œ ì—ëŸ¬ ë°©ì§€ìš©]
ë‹µë³€ ì‹œ ì•„ë˜ ë‘ ê°€ì§€ í˜•ì‹ ì¤‘ í•˜ë‚˜ë¥¼ ë°˜ë“œì‹œ ì„ íƒí•˜ì„¸ìš”.

**ìƒí™© 1. ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°:**
Question: (ì§ˆë¬¸)
Thought: ì¶”ê°€ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤.
Action: {tool_names}
Action Input: (ê²€ìƒ‰ì–´)
Observation: (ê²°ê³¼)

**ìƒí™© 2. ì •ë³´ê°€ ì¶©ë¶„í•  ê²½ìš° (ë‹µë³€ ì‘ì„±):**
Question: (ì§ˆë¬¸)
Thought: ë‚´ë¶€ ì§€ì¹¨ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. (ì´ ì¤„ì— ë‹µë³€ ì“°ì§€ ë§ˆì„¸ìš”!)
Final Answer: ì²« ë¬¸ì¥ì€ ìƒí™©ì— ë§ê²Œ ìœ ì—°í•˜ê²Œ í•˜ì„¸ìš”.

---

[ë‹µë³€ ìŠ¤íƒ€ì¼ ë° ì–¸ì–´ ê°€ì´ë“œ] **(ë§¤ìš° ì¤‘ìš”)**
1. **ì ˆëŒ€ í•œì(Chinese characters)ë‚˜ ì¤‘êµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
   - ì˜ˆ: 'ë¬¼æ ·' -> 'ë¬¼ ê°™ì€', 'å‰§ç—›' -> 'ì‹¬í•œ í†µì¦', 'å¾ˆå¿«' -> 'ë¹ ë¥´ê²Œ'
   - ëª¨ë“  ì „ë¬¸ ìš©ì–´ëŠ” **í•œê¸€**ë¡œ í’€ì–´ì„œ ì“°ì„¸ìš”.
2. **ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì‚¬ìš©**:
   - ê¸°ê³„ì ì¸ ë²ˆì—­íˆ¬("ë‹¹ì‹ ì˜ ì‚¬ë‘ë°›ëŠ” ë°˜ë ¤ê²¬ì„ ìœ„í•´...")ë¥¼ í”¼í•˜ì„¸ìš”.
   - ì‹¤ì œ í•œêµ­ ë™ë¬¼ë³‘ì› ìˆ˜ì˜ì‚¬ ì„ ìƒë‹˜ì²˜ëŸ¼ **"~í•´ ì£¼ì‹œëŠ” ê²Œ ì¢‹ì•„ìš”", "~ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ìš”"** ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ì„¸ìš”.
3. **ê°€ë…ì„±**:
   - ì¤„ê¸€ë³´ë‹¤ëŠ” **ë²ˆí˜¸(1., 2.)**ë¥¼ ì‚¬ìš©í•´ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
   - í•µì‹¬ ë‚´ìš©ì€ **ë³¼ë“œì²´**ë¡œ ê°•ì¡°í•˜ì„¸ìš”.
   - ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬(1~2ê°œ ì •ë„) ë”±ë”±í•˜ì§€ ì•Šê²Œ í•´ ì£¼ì„¸ìš”.

[ì§„ë‹¨ ìš”ì•½]
{context}

[ì›ë‚´ ì˜í•™ ì§€ì¹¨ (ìš°ì„  ì°¸ê³ )]
{local_knowledge}

[ì´ì „ ëŒ€í™”]
{chat_history}

---
ìœ„ ê·œì¹™ì„ ì² ì €íˆ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”. íŠ¹íˆ **Final Answer:** ë’¤ì— í•œê¸€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

Question: {input}
Thought: {agent_scratchpad}
"""
        self.prompt = PromptTemplate.from_template(self.template)

        # 4. ì—ì´ì „íŠ¸ ìƒì„±
        self.agent = create_react_agent(self.llm, self.tools, self.prompt)
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=3,
        )

    def start_case(
        self,
        case_id: str,
        diagnosis: str,
        report_text: str,
        symptoms: Optional[List[str]] = None,
        image_path: Optional[str] = None, # í˜¸í™˜ì„± ìœ ì§€ìš©
    ) -> DogEyeCase:
        """
        ìƒˆë¡œìš´ ì¼€ì´ìŠ¤ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        return DogEyeCase(
            case_id=case_id,
            diagnosis=diagnosis,
            report_text=report_text,
            symptoms=symptoms or []
        )

    def answer(self, case_id: str, question: str, case: DogEyeCase, chat_history_str: str) -> str:
        """
        LangChain Agentë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # Context êµ¬ì„± (ì§„ë‹¨ ì •ë³´ + ë¦¬í¬íŠ¸ ë‚´ìš©)
        diag_info = f"""
- ì§„ë‹¨ëª…: {case.diagnosis}
- ì¦ìƒ: {', '.join(case.symptoms)}
- ë‚´ë¶€ ë¦¬í¬íŠ¸ ìš”ì•½: {case.report_text[:500]}...
"""
        # Local Knowledge ë¡œë“œ
        local_text = load_local_knowledge(case.diagnosis)

        try:
            response = self.agent_executor.invoke({
                "input": question,
                "context": diag_info,
                "local_knowledge": local_text,
                "chat_history": chat_history_str
            })
            return response["output"]
        except Exception as e:
            print(f"Agent Execution Error: {e}")
            return "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. ğŸ˜¢"