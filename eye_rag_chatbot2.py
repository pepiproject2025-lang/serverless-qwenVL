#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
eye_rag_chatbot2.py
"""

import os
import torch
import re
from typing import Any, List, Optional, Dict, Tuple
from dataclasses import dataclass, field

# Transformers
from transformers import AutoModelForVision2Seq, AutoProcessor

# LangChain Core
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.prompts import PromptTemplate

# LangChain Community Tools
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_community.tools import DuckDuckGoSearchResults

# â­ï¸ [Pod í™˜ê²½ í‘œì¤€] langchain_classic ì‚¬ìš©
from langchain_classic.agents import create_react_agent, AgentExecutor

# ----------------------------------
# ì „ì—­ ì„¤ì • & ëª¨ë¸ ìºì‹±
# ----------------------------------
MODEL_DIR = "/runpod-volume/models/Qwen3_VL_8B_Instruct"
CORPUS_DIR = "/runpod-volume/corpus/"

_GLOBAL_MODEL = None
_GLOBAL_PROCESSOR = None

def load_global_model():
    global _GLOBAL_MODEL, _GLOBAL_PROCESSOR
    if _GLOBAL_MODEL is not None:
        return _GLOBAL_MODEL, _GLOBAL_PROCESSOR

    print(f"Loading Model from {MODEL_DIR}...")
    try:
        model = AutoModelForVision2Seq.from_pretrained(
            MODEL_DIR, device_map="auto", torch_dtype="auto", trust_remote_code=True
        ).eval()
        processor = AutoProcessor.from_pretrained(MODEL_DIR, trust_remote_code=True)
        print("Model Loaded Successfully!")
        _GLOBAL_MODEL = model
        _GLOBAL_PROCESSOR = processor
        return model, processor
    except Exception as e:
        print(f"Model Load Failed: {e}")
        raise e

# ----------------------------------
# [í•„ìˆ˜] ë ˆê±°ì‹œ ë°ì´í„° í´ë˜ìŠ¤ ë³µêµ¬
# ----------------------------------
@dataclass
class AppConfig:
    use_ddg: bool = True
    use_wiki: bool = True
    k: int = 12
    wiki_pages: int = 1
    corpus_dir: str = "/runpod-volume/corpus"
    qwen_local_model_dir: str = "/runpod-volume/models/Qwen3_VL_8B_Instruct"
    answer_max_lines: int = 5
    max_history_turns: int = 4

    @classmethod
    def from_env(cls) -> "AppConfig":
        return cls()

@dataclass
class DogEyeCase:
    case_id: str
    diagnosis: str
    report_text: str
    symptoms: List[str] = field(default_factory=list)
    image_path: Optional[str] = None
    history: List[Dict[str, str]] = field(default_factory=list)

@dataclass
class ChatbotState:
    config: AppConfig
    local_by_diag: Dict[str, Any] = field(default_factory=dict)
    cases: Dict[str, DogEyeCase] = field(default_factory=dict)

def create_chatbot_state(config: Optional[AppConfig] = None) -> ChatbotState:
    return ChatbotState(config=config or AppConfig())

# ----------------------------------
# [í•„ìˆ˜] ë ˆê±°ì‹œ í•¨ìˆ˜ ë³µêµ¬ (ì„í¬íŠ¸ ì—ëŸ¬ ë°©ì§€ìš©)
# ì‹¤ì œ ë¡œì§ì€ ì•ˆ ì“°ì´ë”ë¼ë„ eye_analysis_moduleì´ importí•˜ë¯€ë¡œ ì¡´ì¬í•´ì•¼ í•¨
# ----------------------------------
def load_local_corpus_by_diag(corpus_dir: str = "./corpus") -> Dict[str, List[Dict[str, Any]]]:
    """ë ˆê±°ì‹œ í˜¸í™˜ìš© ë”ë¯¸ í•¨ìˆ˜"""
    return {}

def split_report_into_docs(case: DogEyeCase, max_chars: int = 900) -> List[Dict[str, Any]]:
    """ë ˆê±°ì‹œ í˜¸í™˜ìš© ë”ë¯¸ í•¨ìˆ˜"""
    return []

def ddg_search(queries: List[str], max_results: int = 6) -> Tuple[List[Dict[str, Any]], List[str]]:
    """ë ˆê±°ì‹œ í˜¸í™˜ìš© ë”ë¯¸ í•¨ìˆ˜"""
    return [], []

def wiki_chunks(query: str, max_pages: int = 1, max_chars: int = 2500) -> List[Dict[str, Any]]:
    """ë ˆê±°ì‹œ í˜¸í™˜ìš© ë”ë¯¸ í•¨ìˆ˜"""
    return []

def build_rag_context(case: DogEyeCase, question: str, local_docs: List[Dict[str, Any]], config: AppConfig) -> List[Dict[str, Any]]:
    """
    eye_analysis_module.pyì—ì„œ ì„í¬íŠ¸í•˜ëŠ” í•¨ìˆ˜. 
    í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ ê¸°ë³¸ êµ¬ì¡°ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.
    """
    return []

def build_ctx_block(ctx_docs: List[Dict[str, Any]]) -> str:
    """ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë°˜í™˜ìœ¼ë¡œ í˜¸í™˜ì„± ìœ ì§€"""
    return ""

def start_case(state: ChatbotState, case_id: str, diagnosis: str, report_text: str, image_path: Optional[str] = None, symptoms: Optional[List[str]] = None) -> DogEyeCase:
    """ë ˆê±°ì‹œ í˜¸í™˜ ë˜í¼: ë‚´ë¶€ì ìœ¼ë¡œ ìƒˆë¡œìš´ ë´‡ ë¡œì§ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì—°ê²°"""
    bot = EyeRAGChatbot2()
    case = bot.start_case(case_id, diagnosis, report_text, symptoms, image_path)
    state.cases[case_id] = case
    return case

def answer_question(state: ChatbotState, case_id: str, question: str, mode: str = "brief") -> str:
    """ë ˆê±°ì‹œ í˜¸í™˜ ë˜í¼: handler.pyê°€ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ê²½ìš°ë¥¼ ëŒ€ë¹„"""
    case = state.cases.get(case_id)
    if not case: return "ì¼€ì´ìŠ¤ ì—†ìŒ"
    
    # íˆìŠ¤í† ë¦¬ ë³€í™˜
    history_str = ""
    for msg in case.history:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "user": history_str += f"User: {content}\n"
        elif role == "assistant": history_str += f"AI: {content}\n"
    
    bot = EyeRAGChatbot2()
    answer = bot.answer(case, question, history_str)
    
    case.history.append({"role": "user", "content": question})
    case.history.append({"role": "assistant", "content": answer})
    return answer

# ----------------------------------
# [ë©”ì¸] ìƒˆë¡œìš´ LangChain ë¡œì§
# ----------------------------------
class QwenVLLLM(LLM):
    model: Any = None
    processor: Any = None

    def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> str:
        messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.processor(text=[text], images=None, videos=None, padding=True, return_tensors="pt").to(self.model.device)
        gen_kwargs = {"max_new_tokens": 1024, "do_sample": True, "temperature": 0.1, "repetition_penalty": 1.1, "top_p": 0.9, **kwargs}
        
        with torch.no_grad():
            generated_ids = self.model.generate(**inputs, **gen_kwargs)
        
        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
        output_text = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

        if stop:
            for s in stop:
                if s in output_text: output_text = output_text.split(s)[0]
        return output_text

    @property
    def _llm_type(self) -> str: return "qwen-vl-custom"

def load_local_knowledge(diagnosis_name: str) -> str:
    filename = f"{diagnosis_name}.txt"
    filepath = os.path.join(CORPUS_DIR, filename)
    try:
        if os.path.exists(filepath):
            with open(filepath, "r", encoding="utf-8") as f: return f.read()
        return "í•´ë‹¹ ì§ˆí™˜ì— ëŒ€í•œ ë‚´ë¶€ ìƒì„¸ ì§€ì¹¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"ë‚´ë¶€ ë¬¸ì„œ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

class EyeRAGChatbot2:
    def __init__(self, config: Optional[AppConfig] = None):
        model, processor = load_global_model()
        self.llm = QwenVLLLM(model=model, processor=processor)
        wrapper = DuckDuckGoSearchAPIWrapper(backend="html", max_results=5)
        self.search_tool = DuckDuckGoSearchResults(api_wrapper=wrapper, source="text")
        self.tools = [self.search_tool]
        
        self.template = """
ë‹¹ì‹ ì€ **'ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ë°˜ë ¤ë™ë¬¼ ì•ˆê³¼ ìˆ˜ì˜ì‚¬ AI'**ì…ë‹ˆë‹¤.
ë³´í˜¸ìì˜ ê±±ì •ì— ê³µê°í•˜ë©°, [ì§„ë‹¨ ìš”ì•½]ê³¼ [ì›ë‚´ ì˜í•™ ì§€ì¹¨]ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

[ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬]
{tools}

[í˜•ì‹ ê°€ì´ë“œ - ì‹œìŠ¤í…œ ì—ëŸ¬ ë°©ì§€ìš©]
ë‹µë³€ ì‹œ ì•„ë˜ ë‘ ê°€ì§€ í˜•ì‹ ì¤‘ í•˜ë‚˜ë¥¼ ë°˜ë“œì‹œ ì„ íƒí•˜ì„¸ìš”.

**ìƒí™© 1. ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°:**
Question: (ì§ˆë¬¸)
Thought: ì¶”ê°€ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤.
Action: {tool_names}
Action Input: (ê²€ìƒ‰ì–´)
Observation: (ê²°ê³¼)

**ìƒí™© 2. ì •ë³´ê°€ ì¶©ë¶„í•  ê²½ìš° (ë‹µë³€ ì‘ì„±):**
Question: (ì§ˆë¬¸)
Thought: ë‚´ë¶€ ì§€ì¹¨ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. (ì´ ì¤„ì— ë‹µë³€ ì“°ì§€ ë§ˆì„¸ìš”!)
Final Answer: ì²« ë¬¸ì¥ì€ ìƒí™©ì— ë§ê²Œ ìœ ì—°í•˜ê²Œ í•˜ì„¸ìš”.

---

[ë‹µë³€ ìŠ¤íƒ€ì¼ ë° ì–¸ì–´ ê°€ì´ë“œ] **(ë§¤ìš° ì¤‘ìš”)**
1. **ì ˆëŒ€ í•œì(Chinese characters)ë‚˜ ì¤‘êµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
   - ì˜ˆ: 'ë¬¼æ ·' -> 'ë¬¼ ê°™ì€', 'å‰§ç—›' -> 'ì‹¬í•œ í†µì¦', 'å¾ˆå¿«' -> 'ë¹ ë¥´ê²Œ'
   - ëª¨ë“  ì „ë¬¸ ìš©ì–´ëŠ” **í•œê¸€**ë¡œ í’€ì–´ì„œ ì“°ì„¸ìš”.
2. **ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì‚¬ìš©**:
   - ê¸°ê³„ì ì¸ ë²ˆì—­íˆ¬ë¥¼ í”¼í•˜ê³ , ë™ë„¤ ìˆ˜ì˜ì‚¬ ì„ ìƒë‹˜ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ì„¸ìš”.
3. **ê°€ë…ì„±**:
   - ë²ˆí˜¸(1., 2.)ì™€ ë³¼ë“œì²´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.

[ì§„ë‹¨ ìš”ì•½]
{context}

[ì›ë‚´ ì˜í•™ ì§€ì¹¨ (ìš°ì„  ì°¸ê³ )]
{local_knowledge}

[ì´ì „ ëŒ€í™”]
{chat_history}

---
ìœ„ ê·œì¹™ì„ ì² ì €íˆ ì§€ì¼œ ë‹µë³€í•˜ì„¸ìš”. íŠ¹íˆ **Final Answer:** ë’¤ì— í•œê¸€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

Question: {input}
Thought: {agent_scratchpad}
"""
        self.prompt = PromptTemplate.from_template(self.template)
        # Pod í™˜ê²½ í‘œì¤€: langchain_classic ì‚¬ìš©
        self.agent = create_react_agent(self.llm, self.tools, self.prompt)
        self.agent_executor = AgentExecutor(agent=self.agent, tools=self.tools, verbose=True, handle_parsing_errors=True, max_iterations=3)

    def start_case(self, case_id, diagnosis, report_text, symptoms=None, image_path=None):
        return DogEyeCase(case_id, diagnosis, report_text, symptoms or [], image_path)

    def answer(self, case, question, chat_history_str):
        diag_info = f"""
- ì§„ë‹¨ëª…: {case.diagnosis}
- ì¦ìƒ: {', '.join(case.symptoms)}
- ë‚´ë¶€ ë¦¬í¬íŠ¸ ìš”ì•½: {case.report_text[:500]}...
"""
        local_text = load_local_knowledge(case.diagnosis)
        try:
            response = self.agent_executor.invoke({
                "input": question,
                "context": diag_info,
                "local_knowledge": local_text,
                "chat_history": chat_history_str
            })
            return response["output"]
        except Exception as e:
            print(f"Agent Error: {e}")
            return "ì£„ì†¡í•´ìš”, ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”. ğŸ˜¢"