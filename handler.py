import os, io, base64, json
from typing import List, Dict, Any
from PIL import Image

# Unsloth FastVisionModel : Qwen2.5-VLì— ë§ëŠ” í—¬í¼
from unsloth import FastVisionModel

import torch
import runpod
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel

# ===== í™˜ê²½ ë³€ìˆ˜ =====
# Runpod ì½˜ì†”ì—ì„œ Volumes : <ë„¤íŠ¸ì›Œí¬ ë³¼ë¥¨> -> Mount Path: /runpod-volume
# worker -> runpod-volumeìœ¼ë¡œ ìˆ˜ì •
MODEL_BASE = os.getenv("MODEL_BASE", "/runpod-volume/models/Qwen3_VL_8B_Instruct")
LORA_PATH = os.getenv("LORA_PATH", "") # ë¡œë¼ ì²´í¬í¬ì¸íŠ¸ ë„£ê¸°
LOAD_4BIT = os.getenv("LOAD_4BIT", "true").lower() == "true"
MERGE_LORA = os.getenv("MERGE_LORA", "false").lower() == "true"     # ë³‘í•© ê³ ì • ì˜µì…˜

MODELS_ROOT = "/runpod-volume/models"

if not os.path.exists(MODELS_ROOT):
    print(f"[CHECK] {MODELS_ROOT} ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
else:
    print(f"[CHECK] {MODELS_ROOT} ëª©ë¡:")
    try:
        for name in os.listdir(MODELS_ROOT):
            print("  -", name)
    except Exception as e:
        print(f"[CHECK] ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

if not os.path.exists(MODEL_BASE):
    print(f"[ERROR] MODEL_BASE ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {MODEL_BASE}")
else:
    print(f"[CHECK] MODEL_BASE ê²½ë¡œ í™•ì¸ë¨: {MODEL_BASE}")

# ===== ëª¨ë¸ ë¡œë“œ =====
print(f"[INIT] MODEL_BASE={MODEL_BASE}")
print(f"[INIT] LORA_PATH={LORA_PATH}")
print(f"[INIT] LOAD_4BIT={LOAD_4BIT}, MERGE_LORA={MERGE_LORA}")

torch.set_grad_enabled(False)

model, tokenizer = FastVisionModel.from_pretrained(
    model_name = MODEL_BASE,
    load_in_4bit = LOAD_4BIT,
)

# LoRA ì ìš© (ê°€ëŠ¥í•œ ê²½ë¡œë“¤ì„ ì•ˆì „í•˜ê²Œ ì‹œë„)
def _apply_lora_if_available(base_model):
    if not LORA_PATH or not os.path.exists(LORA_PATH):
        print("[LORA] path missing or not found, skip.")
        return base_model
    
    print(f"[LORA] Trying to load LoRA from: {LORA_PATH}")
    # 1) ìš°ì„  PeftModel ê²½ë¡œ (ì¼ë°˜ì )
    try:
        peft_model = PeftModel.from_pretrained(base_model, LORA_PATH)
        peft_model.eval()
        print("[LORA] Loaded via PeftModel.from_pretrained.")
        return peft_model
    except Exception as e:
        print(f"[LORA] Peft load failed: {e}")

    # 2) Unsloth ë˜í¼ì— load_adapterê°€ ìˆëŠ” ê²½ìš°
    try:
        if hasattr(base_model, "load_adapter"):
            base_model.load_adapter(LORA_PATH)
            print("[LORA] Loaded via base_model.load_adapter.")
            return base_model
    except Exception as e:
        print(f"[LORA] base_model.load_adapter failed: {e}")

    print("[LORA] No LoRA applied.")
    return base_model

model = _apply_lora_if_available(model)

# (ì„ íƒ) ë°°í¬ ê³ ì •/ì†ë„ ê°œì„ : LoRA ë³‘í•©
# if MERGE_LORA:
#     try:
#         model = model.merge_and_unload()
#         print("[LORA] merge_and_upload done.")
#     except Exception as e:
#         print(f"[LORA] merge failed: {e}")

# ===== LoRA on/off ì œì–´ =====
HAS_LORA = isinstance(model, PeftModel)

# PeftModel ì•ˆì— ë¡œë“œëœ ì–´ëŒ‘í„° ì´ë¦„ (ëŒ€ë¶€ë¶„ "default" í•˜ë‚˜ì„)
DEFAULT_ADAPTER_NAME = None
if HAS_LORA:
    peft_config = getattr(model, "peft_config", None)
    if isinstance(peft_config, dict) and peft_config:
        DEFAULT_ADAPTER_NAME = list(peft_config.keys())[0]
        print(f"[LORA] available adapter = {DEFAULT_ADAPTER_NAME}")

def set_lora(enabled: bool):
    """
    ìš”ì²­ë§ˆë‹¤ LoRA ì‚¬ìš© ì—¬ë¶€ë¥¼ ë°”ê¾¸ê¸° ìœ„í•œ í—¬í¼.
    PeftModel ì˜ set_adapter / disable_adapter ë¥¼ ì´ìš©í•œë‹¤.
    """
    if not HAS_LORA:
        return

    try:
        if enabled:
            name = DEFAULT_ADAPTER_NAME or "default"
            if hasattr(model, "set_adapter"):
                model.set_adapter(name)
            print(f"[LORA] enabled (adapter={name})")
        else:
            if hasattr(model, "disable_adapter"):
                model.disable_adapter()
                print("[LORA] disabled via disable_adapter()")
            else:
                # disable_adapter ì—†ëŠ” êµ¬ë²„ì „ì´ë©´ ì™„ì „íˆ ë„ì§€ ëª»í•  ìˆ˜ë„ ìˆìŒ
                print("[LORA] disable_adapter() not found, cannot fully disable.")
    except Exception as e:
        print(f"[LORA] set_lora({enabled}) failed: {e}")

# ì´ˆê¸° ìƒíƒœëŠ” 'ì§„ë‹¨ ëª¨ë“œ' ê¸°ì¤€ìœ¼ë¡œ LoRA ì¼œë‘ê¸°
set_lora(True)

FastVisionModel.for_inference(model)
processor = AutoProcessor.from_pretrained(MODEL_BASE)

# ===== ìœ í‹¸ =====
def _normalize_github_url(url: str) -> str:
    # blob â†’ raw, refs/heads â†’ branch ì´ë¦„ ì •ë¦¬, ì¿¼ë¦¬ ì œê±°
    if "github.com" in url and "/blob/" in url:
        url = url.replace("https://github.com/", "https://raw.githubusercontent.com/").replace("/blob/", "/")
    url = url.replace("/refs/heads/", "/")
    if "raw.githubusercontent.com" in url and "?" in url:
        url = url.split("?", 1)[0]
    return url

def _load_image(x: str) -> Image.Image:
    if x.startswith(("http://", "https://")):
        import requests
        url = _normalize_github_url(x)
        try:
            r = requests.get(url, timeout=10, allow_redirects=True)
            status = r.status_code
            ctype = r.headers.get("Content-Type", "")
            print(f"[IMAGE] GET {url} -> {status} {ctype}")
            # ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ ë³¸ë¬¸ ì¼ë¶€ë¥¼ ê°™ì´ ì¶œë ¥
            if not ctype.lower().startswith("image/"):
                snippet = r.text[:200].replace("\n"," ")
                raise RuntimeError(f"Non-image content: status={status}, content-type={ctype}, body[:200]={snippet}")
            r.raise_for_status()
            return Image.open(io.BytesIO(r.content)).convert("RGB")
        except Exception as e:
            raise RuntimeError(f"Image fetch failed: {url} | {type(e).__name__}: {e}")
    # base64
    try:
        return Image.open(io.BytesIO(base64.b64decode(x))).convert("RGB")
    except Exception as e:
        raise RuntimeError(f"Invalid base64 image: {type(e).__name__}: {e}")


def _count_image_tokens(txt: str) -> int:
    return txt.count("<|image_pad|>")

def _infer(prompt: str, images: List[str], gen: Dict[str, Any] | None):
    pil_imgs = [_load_image(s) for s in images]

    # 1) ìš°ì„  ê³µì‹ í…œí”Œë¦¿ ì‹œë„
    messages = [{
        "role": "user",
        "content": [
            *([{"type": "image", "image": img} for img in pil_imgs]),
            {"type": "text", "text": prompt},
        ],
    }]

    try:
        text = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=False,
        )
    except Exception:
        text = None

    # 2) í…œí”Œë¦¿ ê²°ê³¼ ì ê²€: ì´ë¯¸ì§€ í† í°ì´ 0ê°œë©´ ìˆ˜ë™ìœ¼ë¡œ ì‚½ì… (í™•ì‹¤í•œ ë°©ë²•)
    if not text or _count_image_tokens(text) == 0:
        image_tokens = "".join(["<|vision_start|><|image_pad|><|vision_end|>" for _ in pil_imgs])
        # (ì„ íƒ) ëŒ€í™” í¬ë§· í† í°ì´ í•„ìš”í•œ ë²„ì „ ëŒ€ë¹„
        text = f"<|im_start|>user\n{image_tokens}\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n"

    # ì•ˆì „ ê²€ì‚¬: ì´ë¯¸ì§€ ê°œìˆ˜ == í† í° ê°œìˆ˜
    n_tok = _count_image_tokens(text)
    assert n_tok == len(pil_imgs), f"image tokens {n_tok} != images {len(pil_imgs)}"

    # 3) ì…ë ¥ ë§Œë“¤ê¸° & ìƒì„±
    inputs = processor(text=text, images=pil_imgs, return_tensors="pt").to(model.device)

    gen = gen or {}
    output = model.generate(
        **inputs,
        max_new_tokens=int(gen.get("max_new_tokens", 512)),
        temperature=float(gen.get("temperature", 0.2)),
        top_p=float(gen.get("top_p", 0.9)),
        top_k=int(gen.get("top_k", 50)),
    )

    # ğŸ”§ í•µì‹¬: ì…ë ¥ ê¸¸ì´ ì´í›„ë§Œ ë””ì½”ë“œ
    input_len = inputs["input_ids"].shape[1]
    gen_only = output[:, input_len:]
    text = tokenizer.decode(gen_only[0], skip_special_tokens=True)
    return text



# ===== Runpod í•¸ë“¤ëŸ¬ =====
def handler(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê¸°ëŒ€ ì…ë ¥(JSON):

    {
      "input": {
        "prompt": "ì§ˆë¬¸ ë˜ëŠ” ì§„ë‹¨ìš© í”„ë¡¬í”„íŠ¸",
        "images": ["<url or base64>", ...],    
        "gen": {"max_new_tokens": 512, "temperature":0.2}                 

        # ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” í•„ë“œ
        "mode": "diag" | "chat",         # ì§„ë‹¨(diag) ë˜ëŠ” ì±—(chat)
        "use_lora": true | false         
      }
    }
    """
    try:
        payload = event.get("input") or event or {}
        prompt = payload.get("prompt", "Describe the image.")
        images = payload.get("images") or []
        gen = payload.get("gen") or {}

        # 1) ëª¨ë“œ & LoRA ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        mode = payload.get("mode", "diag")  # "diag" or "chat"
        use_lora_flag = payload.get("use_lora")
        if use_lora_flag is None:
            # ëª…ì‹œ ì•ˆ í•´ì£¼ë©´: ì§„ë‹¨(diag)ë§Œ LoRA ì‚¬ìš©, ì±—(chat)ì€ LoRA ë”
            use_lora_flag = (mode == "diag")

        if HAS_LORA:
            set_lora(bool(use_lora_flag))
            print(f"[HANDLER] mode={mode}, use_lora={use_lora_flag}")
        else:
            print(f"[HANDLER] mode={mode}, HAS_LORA=False")

        # 2) ëª¨ë“œë³„ ê¸°ë³¸ ê²€ì¦
        if mode == "diag" and not images:
            # ì§„ë‹¨ ëª¨ë“œì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ ë°˜ë“œì‹œ ìš”êµ¬
            return {"error": "images required (url or base64 list) for diag mode"}

        # chat ëª¨ë“œì—ì„œëŠ” images ì—†ì–´ë„ _infer ê°€ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë™ì‘í•¨
        result = _infer(prompt, images, gen)

        return {
            "output": result,
            "mode": mode,
            "used_lora": bool(use_lora_flag) and HAS_LORA,
        }

    except Exception as e:
        import traceback
        tb = traceback.format_exc()[:4000]
        print("[ERROR]", tb)
        return {"error": f"{e}", "trace": tb}
    

if __name__ == "__main__":
    # Runpod serverless runtime
    runpod.serverless.start({"handler": handler})
