import os, io, base64, json
from typing import List, Dict, Any
from PIL import Image

# Unsloth FastVisionModel : Qwen2.5-VLì— ë§ëŠ” í—¬í¼
from unsloth import FastVisionModel

import torch
import runpod
from transformers import AutoProcessor
from peft import PeftModel

# ===== í™˜ê²½ ë³€ìˆ˜ =====
# Runpod ì½˜ì†”ì—ì„œ Volumes : <ë„¤íŠ¸ì›Œí¬ ë³¼ë¥¨> -> Mount Path: /runpod-volume
# worker -> runpod-volumeìœ¼ë¡œ ìˆ˜ì •
MODEL_BASE = os.getenv("MODEL_BASE", "/runpod-volume/models/Qwen2.5_VL_7B_Instruct")
LORA_PATH = os.getenv("LORA_PATH", "/runpod-volume/outputs/checkpoint-11200")
LOAD_4BIT = os.getenv("LOAD_4BIT", "true").lower() == "true"
MERGE_LORA = os.getenv("MERGE_LORA", "false").lower() == "true"     # ë³‘í•© ê³ ì • ì˜µì…˜

# ===== ëª¨ë¸ ë¡œë“œ =====
print(f"[INIT] MODEL_BASE={MODEL_BASE}")
print(f"[INIT] LORA_PATH={LORA_PATH}")
print(f"[INIT] LOAD_4BIT={LOAD_4BIT}, MERGE_LORA={MERGE_LORA}")

torch.set_grad_enabled(False)

model, tokenizer = FastVisionModel.from_pretrained(
    model_name = MODEL_BASE,
    load_in_4bit = LOAD_4BIT,
)

# LoRA ì ìš© (ê°€ëŠ¥í•œ ê²½ë¡œë“¤ì„ ì•ˆì „í•˜ê²Œ ì‹œë„)
def _apply_lora_if_available(base_model):
    if not LORA_PATH or not os.path.exists(LORA_PATH):
        print("[LORA] path missing or not found, skip.")
        return base_model
    
    print(f"[LORA] Trying to load LoRA from: {LORA_PATH}")
    # 1) ìš°ì„  PeftModel ê²½ë¡œ (ì¼ë°˜ì )
    try:
        peft_model = PeftModel.from_pretrained(base_model, LORA_PATH)
        peft_model.eval()
        print("[LORA] Loaded via PeftModel.from_pretrained.")
        return peft_model
    except Exception as e:
        print(f"[LORA] Peft load failed: {e}")

    # 2) Unsloth ë˜í¼ì— load_adapterê°€ ìˆëŠ” ê²½ìš°
    try:
        if hasattr(base_model, "load_adapter"):
            base_model.load_adapter(LORA_PATH)
            print("[LORA] Loaded via base_model.load_adapter.")
            return base_model
    except Exception as e:
        print(f"[LORA] base_model.load_adapter failed: {e}")

    print("[LORA] No LoRA applied.")
    return base_model

model = _apply_lora_if_available(model)

# (ì„ íƒ) ë°°í¬ ê³ ì •/ì†ë„ ê°œì„ : LoRA ë³‘í•©
# if MERGE_LORA:
#     try:
#         model = model.merge_and_unload()
#         print("[LORA] merge_and_upload done.")
#     except Exception as e:
#         print(f"[LORA] merge failed: {e}")

FastVisionModel.for_inference(model)
processor = AutoProcessor.from_pretrained(MODEL_BASE)

# ===== ìœ í‹¸ =====
def _normalize_github_url(url: str) -> str:
    # blob â†’ raw, refs/heads â†’ branch ì´ë¦„ ì •ë¦¬, ì¿¼ë¦¬ ì œê±°
    if "github.com" in url and "/blob/" in url:
        url = url.replace("https://github.com/", "https://raw.githubusercontent.com/").replace("/blob/", "/")
    url = url.replace("/refs/heads/", "/")
    if "raw.githubusercontent.com" in url and "?" in url:
        url = url.split("?", 1)[0]
    return url

def _load_image(x: str) -> Image.Image:
    if x.startswith(("http://", "https://")):
        import requests
        url = _normalize_github_url(x)
        try:
            r = requests.get(url, timeout=10, allow_redirects=True)
            status = r.status_code
            ctype = r.headers.get("Content-Type", "")
            print(f"[IMAGE] GET {url} -> {status} {ctype}")
            # ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ ë³¸ë¬¸ ì¼ë¶€ë¥¼ ê°™ì´ ì¶œë ¥
            if not ctype.lower().startswith("image/"):
                snippet = r.text[:200].replace("\n"," ")
                raise RuntimeError(f"Non-image content: status={status}, content-type={ctype}, body[:200]={snippet}")
            r.raise_for_status()
            return Image.open(io.BytesIO(r.content)).convert("RGB")
        except Exception as e:
            raise RuntimeError(f"Image fetch failed: {url} | {type(e).__name__}: {e}")
    # base64
    try:
        return Image.open(io.BytesIO(base64.b64decode(x))).convert("RGB")
    except Exception as e:
        raise RuntimeError(f"Invalid base64 image: {type(e).__name__}: {e}")


def _count_image_tokens(txt: str) -> int:
    return txt.count("<|image_pad|>")

def _infer(prompt: str, images: List[str], gen: Dict[str, Any] | None):
    pil_imgs = [_load_image(s) for s in images]

    # 1) ìš°ì„  ê³µì‹ í…œí”Œë¦¿ ì‹œë„
    messages = [{
        "role": "user",
        "content": [
            *([{"type": "image", "image": img} for img in pil_imgs]),
            {"type": "text", "text": prompt},
        ],
    }]

    try:
        text = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=False,
        )
    except Exception:
        text = None

    # 2) í…œí”Œë¦¿ ê²°ê³¼ ì ê²€: ì´ë¯¸ì§€ í† í°ì´ 0ê°œë©´ ìˆ˜ë™ìœ¼ë¡œ ì‚½ì… (í™•ì‹¤í•œ ë°©ë²•)
    if not text or _count_image_tokens(text) == 0:
        image_tokens = "".join(["<|vision_start|><|image_pad|><|vision_end|>" for _ in pil_imgs])
        # (ì„ íƒ) ëŒ€í™” í¬ë§· í† í°ì´ í•„ìš”í•œ ë²„ì „ ëŒ€ë¹„
        text = f"<|im_start|>user\n{image_tokens}\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n"

    # ì•ˆì „ ê²€ì‚¬: ì´ë¯¸ì§€ ê°œìˆ˜ == í† í° ê°œìˆ˜
    n_tok = _count_image_tokens(text)
    assert n_tok == len(pil_imgs), f"image tokens {n_tok} != images {len(pil_imgs)}"

    # 3) ì…ë ¥ ë§Œë“¤ê¸° & ìƒì„±
    inputs = processor(text=text, images=pil_imgs, return_tensors="pt").to(model.device)

    gen = gen or {}
    output = model.generate(
        **inputs,
        max_new_tokens=int(gen.get("max_new_tokens", 512)),
        temperature=float(gen.get("temperature", 0.2)),
        top_p=float(gen.get("top_p", 0.9)),
        top_k=int(gen.get("top_k", 50)),
    )

    # ğŸ”§ í•µì‹¬: ì…ë ¥ ê¸¸ì´ ì´í›„ë§Œ ë””ì½”ë“œ
    input_len = inputs["input_ids"].shape[1]
    gen_only = output[:, input_len:]
    text = tokenizer.decode(gen_only[0], skip_special_tokens=True)
    return text



# ===== Runpod í•¸ë“¤ëŸ¬ =====
def handler(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê¸°ëŒ€ ì…ë ¥(JSON):
    {
        "input": {
            "prompt": "...",
            "images": ["<url or base64>", "..."],
            "gen": {"max_new_tokens": 512, "temperature":0.2}
        }
    }
    """
    try:
        payload = event.get("input", {}) if isinstance(event, dict) else {}
        prompt  = payload.get("prompt", "Describe the image.")
        images  = payload.get("images", [])
        gen     = payload.get("gen", {})

        if not images:
            return {"error": "images required (url or base64 list)"}
        
        result = _infer(prompt, images, gen)
        return {"output": result}
    except Exception as e:
        import traceback
        tb = traceback.format_exc()[:4000]
        print("[ERROR]", tb)
        return {"error": f"{e}", "trace": tb}
    

if __name__ == "__main__":
    # Runpod serverless runtime
    runpod.serverless.start({"handler": handler})
